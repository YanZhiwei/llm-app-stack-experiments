import argparse
import logging
import os
import sys

from agent.graph import graph
from agent.state import OverallState
from langchain_core.messages import HumanMessage

sys.path.insert(0, os.path.abspath(
    os.path.join(os.path.dirname(__file__), '../src')))


def check_env():
    """æ£€æŸ¥å¿…éœ€çš„ç¯å¢ƒå˜é‡ã€‚"""
    required_azure_vars = [
        "AZURE_OPENAI_API_KEY",
        "AZURE_OPENAI_ENDPOINT",
        "AZURE_OPENAI_API_VERSION",
        "AZURE_OPENAI_CHAT_DEPLOYMENT_NAME"
    ]
    missing = [var for var in required_azure_vars if not os.getenv(var)]
    if missing:
        print(f"[ç¯å¢ƒå˜é‡ç¼ºå¤±] ç¼ºå°‘: {', '.join(missing)}")
        exit(1)
    # Tavily å¯é€‰
    if not os.getenv("TAVILY_API_KEY"):
        print("[è­¦å‘Š] æœªé…ç½® TAVILY_API_KEYï¼Œéƒ¨åˆ†æœç´¢åŠŸèƒ½å°†ä¸å¯ç”¨ã€‚")


def main() -> None:
    """ä»å‘½ä»¤è¡Œè¿è¡Œç ”ç©¶ä»£ç†ã€‚"""
    check_env()
    os.environ["LANGCHAIN_TRACING_V2"] = "false"
    os.environ["LANGCHAIN_ENDPOINT"] = ""
    logging.basicConfig(level=logging.INFO,
                        format="%(asctime)s %(levelname)s %(message)s")
    logger = logging.getLogger(__name__)
    parser = argparse.ArgumentParser(
        description="è¿è¡Œ LangGraph ç ”ç©¶ä»£ç†")
    parser.add_argument("question", nargs="?", default=None,
                        help="ç ”ç©¶é—®é¢˜")
    parser.add_argument(
        "--initial-queries",
        type=int,
        default=3,
        help="åˆå§‹æœç´¢æŸ¥è¯¢æ•°é‡",
    )
    parser.add_argument(
        "--max-loops",
        type=int,
        default=2,
        help="æœ€å¤§ç ”ç©¶å¾ªç¯æ•°",
    )
    parser.add_argument(
        "--reasoning-model",
        default="gpt-4o-mini",  # é»˜è®¤ç”¨ AzureOpenAI éƒ¨ç½²å
        help="æœ€ç»ˆç­”æ¡ˆçš„æ¨¡å‹",
    )
    args = parser.parse_args()

    # å¦‚æœæ²¡æœ‰å‚æ•°ï¼Œè‡ªåŠ¨åŠ ä¸Šé»˜è®¤é—®é¢˜
    if len(sys.argv) == 1:
        sys.argv += ["äººå·¥æ™ºèƒ½åœ¨åŒ»ç–—è¯Šæ–­ä¸­çš„åº”ç”¨"]
    question = args.question or "äººå·¥æ™ºèƒ½åœ¨åŒ»ç–—è¯Šæ–­ä¸­çš„åº”ç”¨"

    logger.info(f"\nğŸ”¬ [deepresearcher] ç ”ç©¶ä¸»é¢˜: {question}")
    logger.info(
        f"å‚æ•°: initial_queries={args.initial_queries}, max_loops={args.max_loops}, model={args.reasoning_model}")

    state: OverallState = {
        "messages": [HumanMessage(content=question)],
        "search_query": [],
        "web_research_result": [],
        "sources_gathered": [],
        "initial_search_query_count": args.initial_queries,
        "max_research_loops": args.max_loops,
        "research_loop_count": 0,
        "reasoning_model": args.reasoning_model,
    }

    # é€æ­¥æ‰“å°æ¯è½®å¾ªç¯çš„è¯¦ç»†æ—¥å¿—
    result = None
    try:
        for step in graph.stream(state):
            loop = step.get("research_loop_count", 0)
            if "search_query" in step and loop > 0:
                logger.info(f"\n{'='*20} ç ”ç©¶å¾ªç¯ #{loop} {'='*20}")
            if "search_query" in step:
                queries = step["search_query"]
                if isinstance(queries, list):
                    logger.info(
                        f"ğŸŒ [deepresearcher] æ‰§è¡Œç½‘ç»œæœç´¢: {', '.join(map(str, queries))}")
                else:
                    logger.info(f"ğŸŒ [deepresearcher] æ‰§è¡Œç½‘ç»œæœç´¢: {queries}")
            if "web_research_result" in step:
                logger.info(
                    f"ğŸŒ [deepresearcher] æœç´¢ç»“æœæ‘˜è¦: {step['web_research_result'][0][:100]}... å…±{len(step.get('sources_gathered', []))}æ¡")
            if "sources_gathered" in step and step["sources_gathered"]:
                logger.info("ğŸŒ [deepresearcher] æœ¬è½®è¯¦ç»†æœç´¢ç»“æœï¼š")
                for idx, src in enumerate(step["sources_gathered"], 1):
                    title = src.get("label", "")
                    url = src.get("value", "")
                    content = src.get("content", "")[:100]
                    logger.info(f"  {idx}. [{title}]({url}) {content}")
            if "is_sufficient" in step:
                logger.info(
                    f"ğŸ¤” [deepresearcher] åæ€: ä¿¡æ¯å……åˆ†={step['is_sufficient']} | çŸ¥è¯†ç¼ºå£={step.get('knowledge_gap', '')} | åç»­æŸ¥è¯¢={step.get('follow_up_queries', [])}")
                if step["is_sufficient"]:
                    logger.info("[deepresearcher] ä¿¡æ¯å·²å……åˆ†ï¼Œå‡†å¤‡ç”Ÿæˆæœ€ç»ˆç­”æ¡ˆã€‚\n")
                else:
                    logger.info("[deepresearcher] ä¿¡æ¯ä¸å……åˆ†ï¼Œç»§ç»­ç ”ç©¶ä¸‹ä¸€è½®ã€‚\n")
            if "messages" in step and step["messages"]:
                logger.info("ğŸ“ [deepresearcher] ç”Ÿæˆæœ€ç»ˆç­”æ¡ˆ...\n")
                result = step
    except Exception as e:
        logger.error(f"[deepresearcher] ç ”ç©¶æµç¨‹å¼‚å¸¸: {e}")
        raise

    if result:
        messages = result.get("messages", [])
        if messages:
            print(
                "\n================= [deepresearcher] æœ€ç»ˆç­”æ¡ˆ =================\n")
            print(messages[-1].content)
            print("\n===========================================================\n")
        else:
            logger.warning("[deepresearcher] æœªç”Ÿæˆæœ€ç»ˆç­”æ¡ˆã€‚")
    else:
        logger.warning("[deepresearcher] ç ”ç©¶æµç¨‹æœªè·å¾—æœ‰æ•ˆç»“æœã€‚")


if __name__ == "__main__":
    main()
