import logging
import os

from agent.configuration import Configuration
from agent.prompts import (
    answer_instructions,
    get_current_date,
    query_writer_instructions,
    reflection_instructions,
    web_searcher_instructions,
)
from agent.state import (
    OverallState,
    QueryGenerationState,
    ReflectionState,
    WebSearchState,
)
from agent.tools_and_schemas import Reflection, SearchQueryList
from agent.utils import (
    get_citations,
    get_research_topic,
    insert_citation_markers,
    resolve_urls,
)
from dotenv import load_dotenv
from langchain_core.messages import AIMessage
from langchain_core.runnables import RunnableConfig
from langchain_openai import AzureChatOpenAI
from langgraph.graph import END, START, StateGraph
from langgraph.types import Send
from tavily import TavilyClient

# æ£€æŸ¥ AzureOpenAI ç›¸å…³ç¯å¢ƒå˜é‡
required_azure_vars = [
    "AZURE_OPENAI_API_KEY",
    "AZURE_OPENAI_ENDPOINT",
    "AZURE_OPENAI_API_VERSION",
    "AZURE_OPENAI_CHAT_DEPLOYMENT_NAME"
]
missing_vars = [var for var in required_azure_vars if not os.getenv(var)]
if missing_vars:
    raise ValueError(
        f"ç¼ºå°‘ Azure OpenAI ç¯å¢ƒå˜é‡: {', '.join(missing_vars)}")


load_dotenv(encoding="utf-8")

# ç§»é™¤æ‰€æœ‰ä¸ GEMINI ç›¸å…³çš„ import
# from google.genai import Client
# from langchain_google_genai import ChatGoogleGenerativeAI

logger = logging.getLogger(__name__)

# Nodes


def create_llm_from_config(configurable):
    # åªæ”¯æŒ AzureOpenAI
    return AzureChatOpenAI(
        api_key=configurable.azure_openai_api_key,
        azure_endpoint=configurable.azure_openai_endpoint,
        api_version=configurable.azure_openai_api_version,
        azure_deployment=configurable.azure_openai_deployment,
        temperature=0.1,
        max_tokens=4000,
    )


def generate_query(state: OverallState, config: RunnableConfig) -> QueryGenerationState:
    """åŸºäºç”¨æˆ·é—®é¢˜ç”Ÿæˆæœç´¢æŸ¥è¯¢çš„ LangGraph èŠ‚ç‚¹ã€‚

    ä½¿ç”¨ Azure OpenAI ä¸ºç½‘ç»œç ”ç©¶åˆ›å»ºä¼˜åŒ–çš„æœç´¢æŸ¥è¯¢ï¼ŒåŸºäºç”¨æˆ·çš„é—®é¢˜ã€‚

    å‚æ•°:
        state: åŒ…å«ç”¨æˆ·é—®é¢˜çš„å½“å‰å›¾çŠ¶æ€
        config: å¯è¿è¡Œé…ç½®ï¼ŒåŒ…æ‹¬ LLM æä¾›å•†è®¾ç½®

    è¿”å›:
        åŒ…å«çŠ¶æ€æ›´æ–°çš„å­—å…¸ï¼ŒåŒ…æ‹¬åŒ…å«ç”ŸæˆæŸ¥è¯¢çš„ search_query é”®
    """
    logger.info("ğŸ” [deepresearcher] ç”Ÿæˆæœç´¢æŸ¥è¯¢...")
    configurable = Configuration.from_runnable_config(config)
    # ä¸å†å¯¹ initial_search_query_count èµ‹å€¼ï¼Œåªè¯»å–
    llm = create_llm_from_config(configurable)
    structured_llm = llm.with_structured_output(SearchQueryList)

    # Format the prompt
    current_date = get_current_date()
    formatted_prompt = query_writer_instructions.format(
        current_date=current_date,
        research_topic=get_research_topic(state["messages"]),
        number_queries=state["initial_search_query_count"],
    )
    # Generate the search queries
    result = structured_llm.invoke(formatted_prompt)
    # åªå…è®¸ BaseModelï¼Œå®‰å…¨è®¿é—® query å­—æ®µ
    queries = []
    if hasattr(result, "query"):
        queries = result.query
    elif isinstance(result, dict) and "query" in result:
        queries = result["query"]
    logger.info(f"âœ… [deepresearcher] ç”ŸæˆæŸ¥è¯¢: {queries}")
    return {"search_query": queries}


async def tavily_search(query, api_key):
    """ä½¿ç”¨ Tavily æ‰§è¡Œç½‘ç»œç ”ç©¶çš„ LangGraph èŠ‚ç‚¹ã€‚

    ä½¿ç”¨ Tavily æ‰§è¡Œç½‘ç»œæœç´¢ã€‚

    å‚æ•°:
        query: è¦æ‰§è¡Œçš„æœç´¢æŸ¥è¯¢ã€‚
        api_key: Tavily API å¯†é’¥ã€‚

    è¿”å›:
        æ¥è‡ª Tavily çš„æœç´¢ç»“æœåˆ—è¡¨ã€‚
    """
    import asyncio

    def _sync_tavily_search(query, api_key):
        client = TavilyClient(api_key=api_key)
        response = client.search(query, search_depth="basic", max_results=3)
        results = []
        for item in response.get('results', []):
            results.append({
                'title': item.get('title', ''),
                'url': item.get('url', ''),
                'content': item.get('content', '')[:500]
            })
        return results

    # Run the blocking Tavily search in a separate thread
    return await asyncio.to_thread(_sync_tavily_search, query, api_key)


def continue_to_web_research(state: QueryGenerationState):
    """å°†æœç´¢æŸ¥è¯¢å‘é€åˆ°ç½‘ç»œç ”ç©¶èŠ‚ç‚¹çš„ LangGraph èŠ‚ç‚¹ã€‚

    è¿™ç”¨äºç”Ÿæˆ n ä¸ªç½‘ç»œç ”ç©¶èŠ‚ç‚¹ï¼Œæ¯ä¸ªæœç´¢æŸ¥è¯¢ä¸€ä¸ªã€‚
    """
    return [
        Send("web_research", {"search_query": search_query, "id": int(idx)})
        for idx, search_query in enumerate(state["search_query"])
    ]


async def web_research(state: WebSearchState, config: RunnableConfig) -> dict:
    # logger.info("ğŸŒ [deepresearcher] æ‰§è¡Œç½‘ç»œæœç´¢...")  # ç§»é™¤å†—ä½™æ—¥å¿—
    configurable = Configuration.from_runnable_config(config)
    query = state["search_query"]
    all_results = []
    if configurable.tavily_api_key:
        results = await tavily_search(query, configurable.tavily_api_key)
        all_results.extend(results)
        sources_gathered = [
            {"label": r["title"], "short_url": r["url"],
                "value": r["url"], "content": r.get("content", "")}
            for r in results
        ]
        modified_text = "\n\n".join([
            f"{r['title']}\n{r['content']} [{r['title']}]({r['url']})" for r in results
        ])
        logger.info(f"âœ… [deepresearcher] è·å¾— {len(all_results)} ä¸ªæœç´¢ç»“æœ")
        return {
            "search_query": [query],
            "web_research_result": [modified_text],
            "sources_gathered": sources_gathered,
        }
    raise ValueError(
        "æœªè®¾ç½® Tavily API å¯†é’¥ã€‚è¯·è®¾ç½® TAVILY_API_KEY ç¯å¢ƒå˜é‡ã€‚")


def reflection(state: OverallState, config: RunnableConfig) -> dict:
    """è¯†åˆ«çŸ¥è¯†ç©ºç™½å¹¶ç”Ÿæˆæ½œåœ¨åç»­æŸ¥è¯¢çš„ LangGraph èŠ‚ç‚¹ã€‚

    åˆ†æå½“å‰æ‘˜è¦ä»¥è¯†åˆ«éœ€è¦è¿›ä¸€æ­¥ç ”ç©¶çš„é¢†åŸŸï¼Œå¹¶ç”Ÿæˆæ½œåœ¨çš„åç»­æŸ¥è¯¢ã€‚
    ä½¿ç”¨ç»“æ„åŒ–è¾“å‡ºæ¥æå– JSON æ ¼å¼çš„åç»­æŸ¥è¯¢ã€‚

    å‚æ•°:
        state: åŒ…å«è¿è¡Œæ‘˜è¦å’Œç ”ç©¶ä¸»é¢˜çš„å½“å‰å›¾çŠ¶æ€
        config: å¯è¿è¡Œé…ç½®ï¼ŒåŒ…æ‹¬ LLM æä¾›å•†è®¾ç½®

    è¿”å›:
        åŒ…å«çŠ¶æ€æ›´æ–°çš„å­—å…¸ï¼ŒåŒ…æ‹¬åŒ…å«ç”Ÿæˆçš„åç»­æŸ¥è¯¢çš„ search_query é”®
    """
    logger.info("ğŸ¤” [deepresearcher] åæ€åˆ†æä¸­...")
    configurable = Configuration.from_runnable_config(config)
    # Increment the research loop count and get the reasoning model
    state["research_loop_count"] = state.get("research_loop_count", 0) + 1
    reasoning_model = state.get(
        "reasoning_model", configurable.reflection_model)

    # Format the prompt
    current_date = get_current_date()
    formatted_prompt = reflection_instructions.format(
        current_date=current_date,
        research_topic=get_research_topic(state["messages"]),
        summaries="\n\n---\n\n".join(state["web_research_result"]),
    )
    # init Reasoning Model
    llm = create_llm_from_config(configurable)
    result = llm.with_structured_output(Reflection).invoke(formatted_prompt)
    # ç±»å‹å®‰å…¨è®¿é—®
    if hasattr(result, "is_sufficient"):
        is_sufficient = result.is_sufficient
        knowledge_gap = result.knowledge_gap
        follow_up_queries = result.follow_up_queries
    elif isinstance(result, dict):
        is_sufficient = result.get("is_sufficient", False)
        knowledge_gap = result.get("knowledge_gap", "")
        follow_up_queries = result.get("follow_up_queries", [])
    else:
        is_sufficient = False
        knowledge_gap = ""
        follow_up_queries = []
    logger.info(
        f"ğŸ” [deepresearcher] åæ€ç»“æœ: ä¿¡æ¯å……åˆ†={is_sufficient} | çŸ¥è¯†ç¼ºå£={knowledge_gap} | åç»­æŸ¥è¯¢={follow_up_queries}")
    return {
        "is_sufficient": is_sufficient,
        "knowledge_gap": knowledge_gap,
        "follow_up_queries": follow_up_queries,
        "research_loop_count": state["research_loop_count"],
        "number_of_ran_queries": len(state["search_query"]),
    }


def evaluate_research(
    state: ReflectionState,
    config: RunnableConfig,
) -> OverallState:
    """ç¡®å®šç ”ç©¶æµç¨‹ä¸­ä¸‹ä¸€æ­¥çš„ LangGraph è·¯ç”±å‡½æ•°ã€‚

    é€šè¿‡å†³å®šæ˜¯ç»§ç»­æ”¶é›†ä¿¡æ¯è¿˜æ˜¯åŸºäºé…ç½®çš„æœ€å¤§ç ”ç©¶å¾ªç¯æ•°æ¥æœ€ç»ˆç¡®å®šæ‘˜è¦ï¼Œ
    ä»è€Œæ§åˆ¶ç ”ç©¶å¾ªç¯ã€‚

    å‚æ•°:
        state: åŒ…å«ç ”ç©¶å¾ªç¯è®¡æ•°çš„å½“å‰å›¾çŠ¶æ€
        config: å¯è¿è¡Œé…ç½®ï¼ŒåŒ…æ‹¬ max_research_loops è®¾ç½®

    è¿”å›:
        æŒ‡ç¤ºä¸‹ä¸€ä¸ªè¦è®¿é—®çš„èŠ‚ç‚¹çš„å­—ç¬¦ä¸²å­—é¢é‡ï¼ˆ"web_research" æˆ– "finalize_summary"ï¼‰
    """
    configurable = Configuration.from_runnable_config(config)
    max_research_loops = (
        state.get("max_research_loops")
        if state.get("max_research_loops") is not None
        else configurable.max_research_loops
    )
    if state["is_sufficient"] or state["research_loop_count"] >= max_research_loops:
        return "finalize_answer"
    else:
        return [
            Send(
                "web_research",
                {
                    "search_query": follow_up_query,
                    "id": state["number_of_ran_queries"] + int(idx),
                },
            )
            for idx, follow_up_query in enumerate(state["follow_up_queries"])
        ]


def finalize_answer(state: OverallState, config: RunnableConfig) -> dict:
    """æœ€ç»ˆç¡®å®šç ”ç©¶æ‘˜è¦çš„ LangGraph èŠ‚ç‚¹ã€‚

    é€šè¿‡å»é‡å’Œæ ¼å¼åŒ–æºï¼Œç„¶åå°†å®ƒä»¬ä¸è¿è¡Œæ‘˜è¦ç»“åˆï¼Œ
    åˆ›å»ºç»“æ„è‰¯å¥½çš„ç ”ç©¶æŠ¥å‘Šï¼Œå¹¶å¸¦æœ‰é€‚å½“çš„å¼•ç”¨ã€‚

    å‚æ•°:
        state: åŒ…å«è¿è¡Œæ‘˜è¦å’Œæ”¶é›†æºçš„å½“å‰å›¾çŠ¶æ€

    è¿”å›:
        åŒ…å«çŠ¶æ€æ›´æ–°çš„å­—å…¸ï¼ŒåŒ…æ‹¬åŒ…å«æ ¼å¼åŒ–æœ€ç»ˆæ‘˜è¦å’Œæºçš„ running_summary é”®
    """
    logger.info("ğŸ“ [deepresearcher] ç”Ÿæˆæœ€ç»ˆç­”æ¡ˆ...")
    configurable = Configuration.from_runnable_config(config)
    reasoning_model = state.get("reasoning_model") or configurable.answer_model

    # Format the prompt
    current_date = get_current_date()
    formatted_prompt = answer_instructions.format(
        current_date=current_date,
        research_topic=get_research_topic(state["messages"]),
        summaries="\n---\n\n".join(state["web_research_result"]),
    )

    # init Reasoning Model, default to Gemini 2.5 Flash
    llm = create_llm_from_config(configurable)
    result = llm.invoke(formatted_prompt)

    # Replace the short urls with the original urls and add all used urls to the sources_gathered
    unique_sources = []
    for source in state["sources_gathered"]:
        if source["short_url"] in result.content:
            result.content = result.content.replace(
                source["short_url"], source["value"]
            )
            unique_sources.append(source)
    logger.info("âœ… [deepresearcher] ç­”æ¡ˆç”Ÿæˆå®Œæˆ")
    return {
        "messages": [AIMessage(content=result.content)],
        "sources_gathered": unique_sources,
    }


# åˆ›å»ºæˆ‘ä»¬çš„ä»£ç†å›¾
builder = StateGraph(OverallState, config_schema=Configuration)

# å®šä¹‰æˆ‘ä»¬å°†åœ¨å…¶é—´å¾ªç¯çš„èŠ‚ç‚¹
builder.add_node("generate_query", generate_query)
builder.add_node("web_research", web_research)
builder.add_node("reflection", reflection)
builder.add_node("finalize_answer", finalize_answer)

# å°†å…¥å£ç‚¹è®¾ç½®ä¸º `generate_query`
# è¿™æ„å‘³ç€è¿™ä¸ªèŠ‚ç‚¹æ˜¯ç¬¬ä¸€ä¸ªè¢«è°ƒç”¨çš„
builder.add_edge(START, "generate_query")
# æ·»åŠ æ¡ä»¶è¾¹ä»¥åœ¨å¹¶è¡Œåˆ†æ”¯ä¸­ç»§ç»­æœç´¢æŸ¥è¯¢
builder.add_conditional_edges(
    "generate_query", continue_to_web_research, ["web_research"]
)
# åæ€ç½‘ç»œç ”ç©¶
builder.add_edge("web_research", "reflection")
# è¯„ä¼°ç ”ç©¶
builder.add_conditional_edges(
    "reflection", evaluate_research, ["web_research", "finalize_answer"]
)
# æœ€ç»ˆç¡®å®šç­”æ¡ˆ
builder.add_edge("finalize_answer", END)

graph = builder.compile(name="pro-search-agent")
