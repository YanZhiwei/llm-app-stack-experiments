"""
AgentèŠ‚ç‚¹å‡½æ•°
"""

import asyncio
import json
import logging
from typing import Dict, List, Optional

from langchain_core.runnables import RunnableConfig

from config import config
from state import AgentState

logger = logging.getLogger(__name__)


def generate_queries(state: AgentState, runnable_config: Optional[RunnableConfig] = None) -> AgentState:
    """ç”Ÿæˆæœç´¢æŸ¥è¯¢"""
    logger.info("ğŸ” ç”Ÿæˆæœç´¢æŸ¥è¯¢...")

    llm = config.create_llm()

    # å¦‚æœæœ‰åç»­æŸ¥è¯¢ï¼Œç›´æ¥ç”¨
    if state.follow_up_queries:
        state.search_queries = state.follow_up_queries
        logger.info(f"ğŸ”„ ä½¿ç”¨åæ€ç”Ÿæˆçš„åç»­æŸ¥è¯¢: {state.search_queries}")
        state.follow_up_queries = []  # ç”¨å®Œå³æ¸…ç©ºï¼Œé˜²æ­¢æ­»å¾ªç¯
        return state

    prompt = f"""
    ä¸ºä»¥ä¸‹ç ”ç©¶ä¸»é¢˜ç”Ÿæˆ3ä¸ªé«˜è´¨é‡çš„æœç´¢æŸ¥è¯¢ï¼š

    ç ”ç©¶ä¸»é¢˜ï¼š{state.research_topic}

    è¯·ç”Ÿæˆ3ä¸ªä¸åŒçš„æœç´¢æŸ¥è¯¢ï¼Œè¦†ç›–ä¸»é¢˜çš„ä¸åŒæ–¹é¢ã€‚
    ä»¥JSONæ ¼å¼è¿”å›ï¼š{{"queries": ["æŸ¥è¯¢1", "æŸ¥è¯¢2", "æŸ¥è¯¢3"]}}
    """

    try:
        response = llm.invoke(prompt)
        content = str(response.content)
        # å°è¯•è§£æJSON
        try:
            queries_json = json.loads(content)
            queries = queries_json.get("queries", [])
        except Exception:
            # å›é€€åˆ°åŸæœ‰è§£æ
            lines = content.split('\n')
            queries = []
            for line in lines:
                if '"' in line and not line.strip().startswith('{') and not line.strip().startswith('}'):
                    query = line.split('"')[1] if '"' in line else line.strip()
                    if query and len(query) > 5:
                        queries.append(query)
        if not queries:
            queries = [
                f"{state.research_topic} æ¦‚è¿°",
                f"{state.research_topic} æœ€æ–°å‘å±•",
                f"{state.research_topic} ä¸“å®¶è§‚ç‚¹"
            ]
        state.search_queries = queries[:3]
        logger.info(
            f"âœ… ç”Ÿæˆäº† {len(state.search_queries)} ä¸ªæŸ¥è¯¢: {state.search_queries}")
    except Exception as e:
        logger.error(f"ç”ŸæˆæŸ¥è¯¢å¤±è´¥: {e}")
        state.search_queries = [
            f"{state.research_topic} æ¦‚è¿°",
            f"{state.research_topic} æœ€æ–°å‘å±•",
            f"{state.research_topic} ä¸“å®¶è§‚ç‚¹"
        ]
    return state


async def search_web(query: str) -> List[Dict]:
    """æ‰§è¡Œç½‘ç»œæœç´¢"""
    try:
        if config.tavily_api_key:
            # ä½¿ç”¨Tavily
            from tavily import TavilyClient
            client = TavilyClient(api_key=config.tavily_api_key)
            response = client.search(
                query, search_depth="basic", max_results=3)

            results = []
            for item in response.get('results', []):
                results.append({
                    'title': item.get('title', ''),
                    'url': item.get('url', ''),
                    'content': item.get('content', '')[:500]
                })
            return results
        else:
            # ä½¿ç”¨DuckDuckGo
            from duckduckgo_search import DDGS
            ddgs = DDGS()
            results = []
            for item in ddgs.text(query, max_results=3):
                results.append({
                    'title': item.get('title', ''),
                    'url': item.get('href', ''),
                    'content': item.get('body', '')[:500]
                })
            return results
    except Exception as e:
        logger.error(f"æœç´¢å¤±è´¥: {e}")
        return []


def web_research(state: AgentState, runnable_config: Optional[RunnableConfig] = None) -> AgentState:
    """æ‰§è¡Œç½‘ç»œç ”ç©¶"""
    logger.info("ğŸŒ æ‰§è¡Œç½‘ç»œç ”ç©¶...")

    all_results = []

    for i, query in enumerate(state.search_queries):
        logger.info(f"æœç´¢ {i+1}/{len(state.search_queries)}: {query}")

        try:
            # åŒæ­¥æ‰§è¡Œå¼‚æ­¥æœç´¢
            results = asyncio.run(search_web(query))
            all_results.extend(results)

        except Exception as e:
            logger.error(f"æŸ¥è¯¢ '{query}' æœç´¢å¤±è´¥: {e}")
            continue

    # å¦‚æœæ˜¯åç»­æœç´¢ï¼Œåˆå¹¶ç»“æœ
    if state.research_loop_count > 0:
        state.search_results.extend(all_results)
        logger.info(f"âœ… åˆå¹¶æœç´¢ç»“æœï¼Œæ€»è®¡ {len(state.search_results)} ä¸ª")
    else:
        state.search_results = all_results
        logger.info(f"âœ… è·å¾— {len(state.search_results)} ä¸ªæœç´¢ç»“æœ")

    return state


def reflection(state: AgentState, runnable_config: Optional[RunnableConfig] = None) -> AgentState:
    """åæ€åˆ†æï¼Œé©±åŠ¨å¤šè½®æ·±åº¦ç ”ç©¶"""
    logger.info("ğŸ¤” åæ€åˆ†æä¸­...")
    state.research_loop_count += 1
    llm = config.create_llm()
    search_summary = ""
    for i, result in enumerate(state.search_results):
        search_summary += f"\næ¥æº {i+1}: {result.get('title', '')}\n{result.get('content', '')[:200]}...\n"
    prompt = f"""
    ä½ æ˜¯ä¸€ä¸ªä¸¥è°¨çš„ç ”ç©¶å‹AIã€‚è¯·åŸºäºä»¥ä¸‹æœç´¢ç»“æœï¼Œè¯„ä¼°å¯¹ç ”ç©¶ä¸»é¢˜çš„ä¿¡æ¯æ˜¯å¦å……åˆ†ï¼Œå¹¶ç»™å‡ºä¸‹ä¸€æ­¥å»ºè®®ï¼š
    ç ”ç©¶ä¸»é¢˜ï¼š{state.research_topic}
    å½“å‰å¾ªç¯ï¼š{state.research_loop_count}/{state.max_research_loops}
    æœç´¢ç»“æœæ‘˜è¦ï¼š{search_summary}
    è¯·ä¸¥æ ¼ä»¥JSONæ ¼å¼è¿”å›ï¼š
    {{
        "is_sufficient": true/false,  // æ˜¯å¦ä¿¡æ¯å……åˆ†
        "knowledge_gap": "çŸ¥è¯†ç¼ºå£æè¿°ï¼Œè‹¥ä¿¡æ¯å……åˆ†å¯å†™'æ— '",
        "follow_up_queries": ["åç»­æŸ¥è¯¢1", "åç»­æŸ¥è¯¢2"], // è‹¥is_sufficientä¸ºfalseï¼Œå¿…é¡»ç»™å‡º2ä¸ªå…·ä½“ã€é’ˆå¯¹çŸ¥è¯†ç¼ºå£çš„åç»­æŸ¥è¯¢
        "quality_score": 0.0-1.0, // ç ”ç©¶è´¨é‡è¯„åˆ†
        "reasoning": "è¯„ä¼°ç†ç”±"
    }}
    æ³¨æ„ï¼šå¦‚æœis_sufficientä¸ºfalseï¼Œfollow_up_queriesä¸èƒ½ä¸ºç©ºï¼Œä¸”å¿…é¡»ä¸knowledge_gapç´§å¯†ç›¸å…³ã€‚
    """
    try:
        response = llm.invoke(prompt)
        content = str(response.content)
        logger.info(f"ğŸ“ åæ€åŸå§‹è¾“å‡º: {content}")
        # å°è¯•è§£æJSON
        try:
            result = json.loads(content)
            state.is_sufficient = bool(result.get("is_sufficient", False))
            state.knowledge_gap = result.get("knowledge_gap", "")
            # æ— è®ºis_sufficientå¦‚ä½•éƒ½èµ‹å€¼follow_up_queries
            state.follow_up_queries = result.get("follow_up_queries", [])
            state.research_quality_score = float(
                result.get("quality_score", 0.5))
            reasoning = result.get("reasoning", "")
        except Exception:
            # å›é€€åˆ°åŸæœ‰è§£æ
            state.is_sufficient = "true" in content.lower() and "is_sufficient" in content
            state.knowledge_gap = ""
            state.follow_up_queries = []
            state.research_quality_score = 0.5
            reasoning = ""
        logger.info(
            f"ğŸ” åæ€ç»“æœ: ä¿¡æ¯å……åˆ†={state.is_sufficient} | çŸ¥è¯†ç¼ºå£={state.knowledge_gap} | åç»­æŸ¥è¯¢={state.follow_up_queries} | è´¨é‡åˆ†={state.research_quality_score:.2f}")
        if reasoning:
            logger.info(f"ğŸ§  åæ€ç†ç”±: {reasoning}")
    except Exception as e:
        logger.error(f"åæ€åˆ†æå¤±è´¥: {e}")
        state.is_sufficient = False
        state.knowledge_gap = ""
        state.follow_up_queries = [
            f"{state.research_topic} å…·ä½“æ¡ˆä¾‹ç ”ç©¶",
            f"{state.research_topic} æŠ€æœ¯æŒ‘æˆ˜å’Œè§£å†³æ–¹æ¡ˆ"
        ]
        state.research_quality_score = 0.5
    return state


def should_continue_research(state: AgentState) -> str:
    """å†³å®šæ˜¯å¦ç»§ç»­ç ”ç©¶"""
    # å¦‚æœä¿¡æ¯å……åˆ†ï¼Œç›´æ¥ç”Ÿæˆç­”æ¡ˆ
    if state.is_sufficient:
        return "generate_answer"

    # å¦‚æœè¾¾åˆ°æœ€å¤§å¾ªç¯æ¬¡æ•°ï¼Œç”Ÿæˆç­”æ¡ˆ
    if state.research_loop_count >= state.max_research_loops:
        logger.info(f"è¾¾åˆ°æœ€å¤§å¾ªç¯æ¬¡æ•° ({state.max_research_loops})ï¼Œç”Ÿæˆæœ€ç»ˆç­”æ¡ˆ")
        return "generate_answer"

    # å¦‚æœæ²¡æœ‰åç»­æŸ¥è¯¢ï¼Œç”Ÿæˆç­”æ¡ˆ
    if not state.follow_up_queries:
        logger.info("æ²¡æœ‰åç»­æŸ¥è¯¢ï¼Œç”Ÿæˆæœ€ç»ˆç­”æ¡ˆ")
        return "generate_answer"

    # ç»§ç»­ç ”ç©¶
    logger.info(
        f"ç»§ç»­ç ”ç©¶ (å¾ªç¯ {state.research_loop_count}/{state.max_research_loops})")
    return "generate_queries"


def generate_answer(state: AgentState, runnable_config: Optional[RunnableConfig] = None) -> AgentState:
    """ç”Ÿæˆæœ€ç»ˆç­”æ¡ˆ"""
    logger.info("ğŸ“ ç”Ÿæˆæœ€ç»ˆç­”æ¡ˆ...")

    llm = config.create_llm()

    # æ„å»ºæœç´¢ç»“æœæ–‡æœ¬
    search_text = ""
    for i, result in enumerate(state.search_results):
        search_text += f"\n\næ¥æº {i+1}:\næ ‡é¢˜: {result.get('title', '')}\nå†…å®¹: {result.get('content', '')}\n"

    prompt = f"""
    åŸºäºä»¥ä¸‹æœç´¢ç»“æœï¼Œä¸ºç ”ç©¶ä¸»é¢˜ç”Ÿæˆä¸€ä¸ªå…¨é¢çš„ç­”æ¡ˆï¼š
    
    ç ”ç©¶ä¸»é¢˜ï¼š{state.research_topic}
    ç ”ç©¶å¾ªç¯æ¬¡æ•°ï¼š{state.research_loop_count}
    ç ”ç©¶è´¨é‡è¯„åˆ†ï¼š{state.research_quality_score:.2f}
    
    æœç´¢ç»“æœï¼š{search_text}
    
    è¯·ç”Ÿæˆä¸€ä¸ªç»“æ„åŒ–çš„ç­”æ¡ˆï¼ŒåŒ…æ‹¬ï¼š
    1. æ¦‚è¿°
    2. ä¸»è¦å‘ç°
    3. å…³é”®è¦ç‚¹
    4. ç»“è®º
    
    è¯·ç¡®ä¿ç­”æ¡ˆå‡†ç¡®ã€å…¨é¢ï¼Œå¹¶åŸºäºæä¾›çš„æœç´¢ç»“æœã€‚
    """

    try:
        response = llm.invoke(prompt)
        state.final_answer = str(response.content)
        logger.info("âœ… ç­”æ¡ˆç”Ÿæˆå®Œæˆ")

    except Exception as e:
        logger.error(f"ç”Ÿæˆç­”æ¡ˆå¤±è´¥: {e}")
        state.final_answer = f"æŠ±æ­‰ï¼Œæ— æ³•ä¸º '{state.research_topic}' ç”Ÿæˆç­”æ¡ˆã€‚è¯·æ£€æŸ¥ç½‘ç»œè¿æ¥å’ŒAPIé…ç½®ã€‚"

    return state
