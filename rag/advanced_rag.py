"""
é«˜çº§RAGç³»ç»Ÿç¤ºä¾‹
åŒ…å«æµå¼å“åº”ã€å¤šç§æ£€ç´¢ç­–ç•¥ã€é‡æ’åºã€å’Œé«˜çº§æ–‡æ¡£å¤„ç†åŠŸèƒ½
"""

import os
import asyncio
from typing import List, Dict, Any, AsyncIterator
from dotenv import load_dotenv

from langchain_core.documents import Document
from langchain_core.prompts import ChatPromptTemplate, PromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough
from langchain_core.callbacks import StreamingStdOutCallbackHandler

from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_community.vectorstores import Chroma
from langchain_community.document_loaders import PyPDFLoader, UnstructuredMarkdownLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain.retrievers import EnsembleRetriever, BM25Retriever
from langchain.retrievers.document_compressors import LLMChainExtractor
from langchain.retrievers import ContextualCompressionRetriever

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

class AdvancedRAGSystem:
    """é«˜çº§RAGç³»ç»Ÿ"""
    
    def __init__(self, 
                 model_name: str = "gpt-4", 
                 temperature: float = 0.1,
                 streaming: bool = True):
        """
        åˆå§‹åŒ–é«˜çº§RAGç³»ç»Ÿ
        
        Args:
            model_name: OpenAIæ¨¡å‹åç§°
            temperature: ç”Ÿæˆæ¸©åº¦
            streaming: æ˜¯å¦å¯ç”¨æµå¼å“åº”
        """
        self.model_name = model_name
        self.temperature = temperature
        self.streaming = streaming
        
        # åˆå§‹åŒ–å›è°ƒå¤„ç†å™¨
        callbacks = [StreamingStdOutCallbackHandler()] if streaming else []
        
        # åˆå§‹åŒ–OpenAIç»„ä»¶
        self.llm = ChatOpenAI(
            model=model_name,
            temperature=temperature,
            streaming=streaming,
            callbacks=callbacks
        )
        
        self.embeddings = OpenAIEmbeddings(
            model="text-embedding-3-large"
        )
        
        # åˆå§‹åŒ–é«˜çº§æ–‡æœ¬åˆ†å‰²å™¨
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000,
            chunk_overlap=200,
            add_start_index=True,
            separators=["\n\n", "\n", " ", ""]
        )
        
        self.vectorstore = None
        self.retriever = None
        self.compression_retriever = None
        self.rag_chain = None
    
    def load_documents_advanced(self, documents_path: str) -> List[Document]:
        """
        é«˜çº§æ–‡æ¡£åŠ è½½ - æ”¯æŒå¤šç§æ–‡ä»¶æ ¼å¼
        
        Args:
            documents_path: æ–‡æ¡£è·¯å¾„
            
        Returns:
            åŠ è½½çš„æ–‡æ¡£åˆ—è¡¨
        """
        documents = []
        
        if os.path.isfile(documents_path):
            # æ ¹æ®æ–‡ä»¶æ‰©å±•åé€‰æ‹©åˆé€‚çš„åŠ è½½å™¨
            if documents_path.endswith('.pdf'):
                loader = PyPDFLoader(documents_path)
            elif documents_path.endswith('.md'):
                loader = UnstructuredMarkdownLoader(documents_path)
            else:
                # é»˜è®¤ä½¿ç”¨æ–‡æœ¬åŠ è½½å™¨
                from langchain_community.document_loaders import TextLoader
                loader = TextLoader(documents_path, encoding='utf-8')
            
            documents = loader.load()
        
        elif os.path.isdir(documents_path):
            # éå†ç›®å½•åŠ è½½å¤šç§æ–‡ä»¶ç±»å‹
            for root, dirs, files in os.walk(documents_path):
                for file in files:
                    file_path = os.path.join(root, file)
                    try:
                        if file.endswith('.pdf'):
                            loader = PyPDFLoader(file_path)
                        elif file.endswith('.md'):
                            loader = UnstructuredMarkdownLoader(file_path)
                        elif file.endswith('.txt'):
                            from langchain_community.document_loaders import TextLoader
                            loader = TextLoader(file_path, encoding='utf-8')
                        else:
                            continue
                        
                        docs = loader.load()
                        documents.extend(docs)
                    except Exception as e:
                        print(f"åŠ è½½æ–‡ä»¶ {file_path} æ—¶å‡ºé”™: {e}")
        
        print(f"æˆåŠŸåŠ è½½ {len(documents)} ä¸ªæ–‡æ¡£")
        return documents
    
    def create_hybrid_retriever(self, documents: List[Document]) -> None:
        """
        åˆ›å»ºæ··åˆæ£€ç´¢å™¨ï¼ˆå‘é‡æ£€ç´¢ + BM25æ£€ç´¢ï¼‰
        
        Args:
            documents: æ–‡æ¡£åˆ—è¡¨
        """
        print("æ­£åœ¨åˆ›å»ºæ··åˆæ£€ç´¢å™¨...")
        
        # åˆ›å»ºå‘é‡å­˜å‚¨
        self.vectorstore = Chroma.from_documents(
            documents=documents,
            embedding=self.embeddings,
            persist_directory="./advanced_chroma_db"
        )
        
        # åˆ›å»ºå‘é‡æ£€ç´¢å™¨
        vector_retriever = self.vectorstore.as_retriever(
            search_type="similarity",
            search_kwargs={"k": 6}
        )
        
        # åˆ›å»ºBM25æ£€ç´¢å™¨
        bm25_retriever = BM25Retriever.from_documents(documents)
        bm25_retriever.k = 6
        
        # åˆ›å»ºé›†æˆæ£€ç´¢å™¨
        self.retriever = EnsembleRetriever(
            retrievers=[vector_retriever, bm25_retriever],
            weights=[0.7, 0.3]  # å‘é‡æ£€ç´¢æƒé‡æ›´é«˜
        )
        
        # åˆ›å»ºå‹ç¼©æ£€ç´¢å™¨ï¼ˆç”¨äºé‡æ’åºå’Œè¿‡æ»¤ï¼‰
        compressor = LLMChainExtractor.from_llm(self.llm)
        self.compression_retriever = ContextualCompressionRetriever(
            base_compressor=compressor,
            base_retriever=self.retriever
        )
        
        print("æ··åˆæ£€ç´¢å™¨åˆ›å»ºå®Œæˆ")
    
    def create_advanced_rag_chain(self) -> None:
        """åˆ›å»ºé«˜çº§RAGé“¾"""
        # é«˜çº§æç¤ºæ¨¡æ¿
        template = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„AIåŠ©æ‰‹ã€‚è¯·åŸºäºä»¥ä¸‹ä¸Šä¸‹æ–‡ä¿¡æ¯è¯¦ç»†å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚

æ³¨æ„äº‹é¡¹ï¼š
1. ä»”ç»†åˆ†æä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œæä¾›å‡†ç¡®ã€è¯¦ç»†çš„å›ç­”
2. å¦‚æœä¸Šä¸‹æ–‡ä¿¡æ¯ä¸è¶³ä»¥å®Œå…¨å›ç­”é—®é¢˜ï¼Œè¯·æ˜ç¡®æŒ‡å‡º
3. å¼•ç”¨ç›¸å…³çš„ä¸Šä¸‹æ–‡ä¿¡æ¯æ¥æ”¯æŒä½ çš„å›ç­”
4. ä¿æŒå›ç­”çš„é€»è¾‘æ€§å’Œè¿è´¯æ€§

ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼š
{context}

é—®é¢˜ï¼š{question}

è¯¦ç»†å›ç­”ï¼š"""
        
        prompt = ChatPromptTemplate.from_template(template)
        
        def format_docs(docs):
            formatted = []
            for i, doc in enumerate(docs, 1):
                content = doc.page_content.strip()
                source = getattr(doc.metadata, 'source', 'æœªçŸ¥æ¥æº')
                formatted.append(f"[æ–‡æ¡£{i}] (æ¥æº: {source})\n{content}")
            return "\n\n".join(formatted)
        
        # åˆ›å»ºé«˜çº§RAGé“¾
        self.rag_chain = (
            {
                "context": self.compression_retriever | format_docs,
                "question": RunnablePassthrough()
            }
            | prompt
            | self.llm
            | StrOutputParser()
        )
        
        print("é«˜çº§RAGé“¾åˆ›å»ºå®Œæˆ")
    
    async def aquery(self, question: str) -> str:
        """
        å¼‚æ­¥æŸ¥è¯¢RAGç³»ç»Ÿ
        
        Args:
            question: ç”¨æˆ·é—®é¢˜
            
        Returns:
            ç”Ÿæˆçš„å›ç­”
        """
        if self.rag_chain is None:
            raise ValueError("RAGé“¾å°šæœªåˆ›å»º")
        
        print(f"\nğŸ” é—®é¢˜: {question}")
        print("æ­£åœ¨æ£€ç´¢ç›¸å…³ä¿¡æ¯...")
        
        # è·å–ç›¸å…³æ–‡æ¡£
        relevant_docs = await self.compression_retriever.aget_relevant_documents(question)
        print(f"âœ… æ£€ç´¢åˆ° {len(relevant_docs)} ä¸ªé«˜è´¨é‡æ–‡æ¡£ç‰‡æ®µ")
        
        if self.streaming:
            print("\nğŸ¤– AIå›ç­”ï¼š")
            print("-" * 50)
            
        # ç”Ÿæˆå›ç­”
        answer = await self.rag_chain.ainvoke(question)
        return answer
    
    def query_with_sources(self, question: str) -> Dict[str, Any]:
        """
        æŸ¥è¯¢å¹¶è¿”å›å¸¦æ¥æºçš„å›ç­”
        
        Args:
            question: ç”¨æˆ·é—®é¢˜
            
        Returns:
            åŒ…å«å›ç­”å’Œæ¥æºçš„å­—å…¸
        """
        if self.compression_retriever is None:
            raise ValueError("æ£€ç´¢å™¨å°šæœªåˆ›å»º")
        
        # è·å–ç›¸å…³æ–‡æ¡£
        relevant_docs = self.compression_retriever.get_relevant_documents(question)
        
        # æ ¼å¼åŒ–ä¸Šä¸‹æ–‡
        context = "\n\n".join([doc.page_content for doc in relevant_docs])
        
        # ç”Ÿæˆå›ç­”
        answer = self.rag_chain.invoke(question)
        
        # æå–æ¥æºä¿¡æ¯
        sources = []
        for doc in relevant_docs:
            source_info = {
                "content": doc.page_content[:200] + "...",
                "metadata": doc.metadata
            }
            sources.append(source_info)
        
        return {
            "question": question,
            "answer": answer,
            "sources": sources,
            "num_sources": len(sources)
        }
    
    def setup_advanced(self, documents_path: str) -> None:
        """
        å®Œæ•´è®¾ç½®é«˜çº§RAGç³»ç»Ÿ
        
        Args:
            documents_path: æ–‡æ¡£è·¯å¾„
        """
        print("ğŸš€ å¼€å§‹è®¾ç½®é«˜çº§RAGç³»ç»Ÿ...")
        
        # 1. åŠ è½½æ–‡æ¡£
        documents = self.load_documents_advanced(documents_path)
        
        # 2. å¤„ç†æ–‡æ¡£
        processed_docs = self.text_splitter.split_documents(documents)
        print(f"æ–‡æ¡£åˆ†å‰²ä¸º {len(processed_docs)} ä¸ªç‰‡æ®µ")
        
        # 3. åˆ›å»ºæ··åˆæ£€ç´¢å™¨
        self.create_hybrid_retriever(processed_docs)
        
        # 4. åˆ›å»ºRAGé“¾
        self.create_advanced_rag_chain()
        
        print("âœ… é«˜çº§RAGç³»ç»Ÿè®¾ç½®å®Œæˆï¼")


async def main():
    """ä¸»å‡½æ•°"""
    # æ£€æŸ¥APIå¯†é’¥
    if not os.getenv("OPENAI_API_KEY"):
        print("âŒ é”™è¯¯: è¯·è®¾ç½®OPENAI_API_KEYç¯å¢ƒå˜é‡")
        return
    
    # åˆ›å»ºé«˜çº§RAGç³»ç»Ÿ
    rag = AdvancedRAGSystem(
        model_name="gpt-4",
        temperature=0.1,
        streaming=True
    )
    
    # ç¤ºä¾‹ï¼šä½¿ç”¨ç¤ºä¾‹æ–‡æ¡£ç›®å½•
    sample_docs_path = "sample_docs"
    
    try:
        # è®¾ç½®RAGç³»ç»Ÿ
        rag.setup_advanced(sample_docs_path)
        
        # ç¤ºä¾‹é—®é¢˜
        sample_questions = [
            "è¯·è¯¦ç»†è§£é‡Šäººå·¥æ™ºèƒ½çš„å‘å±•å†å²",
            "æœºå™¨å­¦ä¹ æœ‰å“ªäº›ä¸»è¦ç®—æ³•ç±»å‹ï¼Ÿ",
            "æ·±åº¦å­¦ä¹ åœ¨è®¡ç®—æœºè§†è§‰ä¸­æœ‰ä»€ä¹ˆåº”ç”¨ï¼Ÿ"
        ]
        
        print("\n" + "="*70)
        print("ğŸ¯ é«˜çº§RAGç³»ç»Ÿæ¼”ç¤º")
        print("="*70)
        
        # è¿è¡Œç¤ºä¾‹é—®é¢˜
        for i, question in enumerate(sample_questions, 1):
            print(f"\nğŸ“ ç¤ºä¾‹ {i}:")
            try:
                await rag.aquery(question)
                print("\n" + "-"*50)
            except Exception as e:
                print(f"âŒ å¤„ç†é—®é¢˜æ—¶å‡ºé”™: {e}")
        
        # äº¤äº’å¼æŸ¥è¯¢
        print(f"\n{'='*70}")
        print("ğŸ’¬ äº¤äº’å¼é—®ç­”æ¨¡å¼ (è¾“å…¥ 'quit' é€€å‡º)")
        print("="*70)
        
        while True:
            question = input("\nè¯·è¾“å…¥æ‚¨çš„é—®é¢˜: ").strip()
            
            if question.lower() in ['quit', 'exit', 'é€€å‡º']:
                print("ğŸ‘‹ å†è§ï¼")
                break
            
            if not question:
                continue
            
            try:
                await rag.aquery(question)
                print("\n" + "-"*50)
            except Exception as e:
                print(f"âŒ ç”Ÿæˆå›ç­”æ—¶å‡ºé”™: {e}")
    
    except Exception as e:
        print(f"âŒ è®¾ç½®RAGç³»ç»Ÿæ—¶å‡ºé”™: {e}")
        print("\nğŸ’¡ æç¤º: è¯·ç¡®ä¿å­˜åœ¨ç¤ºä¾‹æ–‡æ¡£ç›®å½•æˆ–ä¿®æ”¹æ–‡æ¡£è·¯å¾„")


if __name__ == "__main__":
    asyncio.run(main()) 