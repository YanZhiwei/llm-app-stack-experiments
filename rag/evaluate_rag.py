"""
RAGç³»ç»Ÿè¯„ä¼°å·¥å…·
ç”¨äºæµ‹è¯•å’Œè¯„ä¼°RAGç³»ç»Ÿçš„æ€§èƒ½å’Œå‡†ç¡®æ€§
"""

import os
import time
from typing import List, Dict, Any
from dotenv import load_dotenv

from quick_start import create_quick_rag

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

class RAGEvaluator:
    """RAGç³»ç»Ÿè¯„ä¼°å™¨"""
    
    def __init__(self):
        """åˆå§‹åŒ–è¯„ä¼°å™¨"""
        self.test_questions = [
            {
                "question": "ä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½ï¼Ÿ",
                "expected_keywords": ["è®¡ç®—æœºç§‘å­¦", "æ¨¡æ‹Ÿ", "äººç±»æ™ºèƒ½", "ç®—æ³•"]
            },
            {
                "question": "æœºå™¨å­¦ä¹ æœ‰å“ªäº›ä¸»è¦ç±»å‹ï¼Ÿ",
                "expected_keywords": ["ç›‘ç£å­¦ä¹ ", "æ— ç›‘ç£å­¦ä¹ ", "å¼ºåŒ–å­¦ä¹ "]
            },
            {
                "question": "æ·±åº¦å­¦ä¹ çš„ä¼˜åŠ¿æ˜¯ä»€ä¹ˆï¼Ÿ",
                "expected_keywords": ["ç¥ç»ç½‘ç»œ", "å¤æ‚æ¨¡å¼", "å›¾åƒè¯†åˆ«", "è¯­éŸ³è¯†åˆ«"]
            },
            {
                "question": "RAGæŠ€æœ¯æ˜¯ä»€ä¹ˆï¼Ÿ",
                "expected_keywords": ["æ£€ç´¢", "ç”Ÿæˆ", "çŸ¥è¯†æˆªæ–­", "å¹»è§‰é—®é¢˜"]
            },
            {
                "question": "å¤§è¯­è¨€æ¨¡å‹æœ‰ä»€ä¹ˆå±€é™æ€§ï¼Ÿ",
                "expected_keywords": ["çŸ¥è¯†æˆªæ–­", "å¹»è§‰é—®é¢˜", "åè§", "è®¡ç®—æˆæœ¬"]
            }
        ]
    
    def evaluate_answer_quality(self, question: str, answer: str, expected_keywords: List[str]) -> Dict[str, Any]:
        """
        è¯„ä¼°å›ç­”è´¨é‡
        
        Args:
            question: é—®é¢˜
            answer: ç”Ÿæˆçš„å›ç­”
            expected_keywords: æœŸæœ›çš„å…³é”®è¯
            
        Returns:
            è¯„ä¼°ç»“æœå­—å…¸
        """
        answer_lower = answer.lower()
        
        # æ£€æŸ¥å…³é”®è¯è¦†ç›–ç‡
        found_keywords = []
        for keyword in expected_keywords:
            if keyword.lower() in answer_lower:
                found_keywords.append(keyword)
        
        keyword_coverage = len(found_keywords) / len(expected_keywords)
        
        # è¯„ä¼°å›ç­”é•¿åº¦
        answer_length = len(answer)
        length_score = min(answer_length / 200, 1.0)  # æœŸæœ›å›ç­”é•¿åº¦è‡³å°‘200å­—ç¬¦
        
        # æ£€æŸ¥å›ç­”æ˜¯å¦åŒ…å«"ä¸çŸ¥é“"ç­‰è¡¨è¾¾
        uncertainty_phrases = ["ä¸çŸ¥é“", "ä¸ç¡®å®š", "æ— æ³•ç¡®å®š", "ä¸æ¸…æ¥š"]
        has_uncertainty = any(phrase in answer for phrase in uncertainty_phrases)
        
        # ç»¼åˆè¯„åˆ†
        overall_score = (keyword_coverage * 0.6 + length_score * 0.3 + (0 if has_uncertainty else 0.1))
        
        return {
            "question": question,
            "answer": answer,
            "keyword_coverage": keyword_coverage,
            "found_keywords": found_keywords,
            "missing_keywords": [kw for kw in expected_keywords if kw not in found_keywords],
            "answer_length": answer_length,
            "length_score": length_score,
            "has_uncertainty": has_uncertainty,
            "overall_score": overall_score
        }
    
    def run_evaluation(self) -> Dict[str, Any]:
        """
        è¿è¡Œå®Œæ•´è¯„ä¼°
        
        Returns:
            è¯„ä¼°ç»“æœ
        """
        print("ğŸ” å¼€å§‹RAGç³»ç»Ÿè¯„ä¼°...")
        print("=" * 50)
        
        try:
            # åˆ›å»ºRAGç³»ç»Ÿ
            print("æ­£åœ¨åˆå§‹åŒ–RAGç³»ç»Ÿ...")
            rag_chain, retriever = create_quick_rag()
            
            results = []
            total_time = 0
            
            for i, test_case in enumerate(self.test_questions, 1):
                question = test_case["question"]
                expected_keywords = test_case["expected_keywords"]
                
                print(f"\nğŸ“ æµ‹è¯•é—®é¢˜ {i}: {question}")
                print("-" * 30)
                
                # è®°å½•å“åº”æ—¶é—´
                start_time = time.time()
                
                try:
                    # è·å–ç›¸å…³æ–‡æ¡£
                    relevant_docs = retriever.get_relevant_documents(question)
                    print(f"æ£€ç´¢åˆ° {len(relevant_docs)} ä¸ªç›¸å…³æ–‡æ¡£")
                    
                    # ç”Ÿæˆå›ç­”
                    answer = rag_chain.invoke(question)
                    
                    end_time = time.time()
                    response_time = end_time - start_time
                    total_time += response_time
                    
                    # è¯„ä¼°å›ç­”è´¨é‡
                    evaluation = self.evaluate_answer_quality(question, answer, expected_keywords)
                    evaluation["response_time"] = response_time
                    evaluation["num_retrieved_docs"] = len(relevant_docs)
                    
                    results.append(evaluation)
                    
                    # æ˜¾ç¤ºç»“æœ
                    print(f"âœ… å›ç­”ç”Ÿæˆå®Œæˆ (ç”¨æ—¶: {response_time:.2f}ç§’)")
                    print(f"å…³é”®è¯è¦†ç›–ç‡: {evaluation['keyword_coverage']:.2%}")
                    print(f"æ‰¾åˆ°çš„å…³é”®è¯: {evaluation['found_keywords']}")
                    if evaluation['missing_keywords']:
                        print(f"é—æ¼çš„å…³é”®è¯: {evaluation['missing_keywords']}")
                    print(f"ç»¼åˆè¯„åˆ†: {evaluation['overall_score']:.2f}/1.0")
                    
                except Exception as e:
                    print(f"âŒ å¤„ç†é—®é¢˜æ—¶å‡ºé”™: {e}")
                    continue
            
            # è®¡ç®—æ€»ä½“ç»Ÿè®¡
            if results:
                avg_score = sum(r['overall_score'] for r in results) / len(results)
                avg_coverage = sum(r['keyword_coverage'] for r in results) / len(results)
                avg_response_time = total_time / len(results)
                
                summary = {
                    "total_questions": len(self.test_questions),
                    "successful_responses": len(results),
                    "success_rate": len(results) / len(self.test_questions),
                    "average_score": avg_score,
                    "average_keyword_coverage": avg_coverage,
                    "average_response_time": avg_response_time,
                    "total_time": total_time,
                    "detailed_results": results
                }
                
                self.print_summary(summary)
                return summary
            else:
                print("âŒ æ²¡æœ‰æˆåŠŸçš„æµ‹è¯•ç»“æœ")
                return {"error": "No successful test results"}
                
        except Exception as e:
            print(f"âŒ è¯„ä¼°è¿‡ç¨‹ä¸­å‡ºé”™: {e}")
            return {"error": str(e)}
    
    def print_summary(self, summary: Dict[str, Any]) -> None:
        """æ‰“å°è¯„ä¼°æ‘˜è¦"""
        print("\n" + "=" * 60)
        print("ğŸ“Š è¯„ä¼°æ‘˜è¦")
        print("=" * 60)
        
        print(f"æ€»é—®é¢˜æ•°: {summary['total_questions']}")
        print(f"æˆåŠŸå›ç­”æ•°: {summary['successful_responses']}")
        print(f"æˆåŠŸç‡: {summary['success_rate']:.2%}")
        print(f"å¹³å‡è¯„åˆ†: {summary['average_score']:.2f}/1.0")
        print(f"å¹³å‡å…³é”®è¯è¦†ç›–ç‡: {summary['average_keyword_coverage']:.2%}")
        print(f"å¹³å‡å“åº”æ—¶é—´: {summary['average_response_time']:.2f}ç§’")
        print(f"æ€»ç”¨æ—¶: {summary['total_time']:.2f}ç§’")
        
        # æ€§èƒ½ç­‰çº§è¯„å®š
        avg_score = summary['average_score']
        if avg_score >= 0.8:
            grade = "ğŸ† ä¼˜ç§€"
        elif avg_score >= 0.6:
            grade = "ğŸ‘ è‰¯å¥½"
        elif avg_score >= 0.4:
            grade = "âš ï¸ ä¸€èˆ¬"
        else:
            grade = "âŒ éœ€è¦æ”¹è¿›"
        
        print(f"ç³»ç»Ÿè¯„çº§: {grade}")
        
        # æ”¹è¿›å»ºè®®
        print("\nğŸ’¡ æ”¹è¿›å»ºè®®:")
        if summary['average_keyword_coverage'] < 0.7:
            print("- è€ƒè™‘ä¼˜åŒ–æ£€ç´¢ç­–ç•¥ï¼Œæé«˜å…³é”®ä¿¡æ¯è¦†ç›–ç‡")
        if summary['average_response_time'] > 10:
            print("- å“åº”æ—¶é—´è¾ƒé•¿ï¼Œè€ƒè™‘ä¼˜åŒ–æ¨¡å‹é…ç½®æˆ–ä½¿ç”¨ç¼“å­˜")
        if summary['success_rate'] < 1.0:
            print("- å­˜åœ¨å¤±è´¥çš„æŸ¥è¯¢ï¼Œæ£€æŸ¥é”™è¯¯å¤„ç†å’Œç³»ç»Ÿç¨³å®šæ€§")
    
    def run_interactive_test(self) -> None:
        """è¿è¡Œäº¤äº’å¼æµ‹è¯•"""
        print("ğŸ¯ äº¤äº’å¼RAGæµ‹è¯•")
        print("=" * 30)
        
        try:
            rag_chain, retriever = create_quick_rag()
            
            print("RAGç³»ç»Ÿå·²å‡†å¤‡å°±ç»ªï¼")
            print("è¾“å…¥é—®é¢˜è¿›è¡Œæµ‹è¯•ï¼Œè¾“å…¥ 'quit' é€€å‡º")
            
            while True:
                question = input("\nè¯·è¾“å…¥æµ‹è¯•é—®é¢˜: ").strip()
                
                if question.lower() in ['quit', 'exit', 'é€€å‡º']:
                    print("æµ‹è¯•ç»“æŸï¼")
                    break
                
                if not question:
                    continue
                
                start_time = time.time()
                
                try:
                    # æ£€ç´¢ç›¸å…³æ–‡æ¡£
                    relevant_docs = retriever.get_relevant_documents(question)
                    
                    # ç”Ÿæˆå›ç­”
                    answer = rag_chain.invoke(question)
                    
                    end_time = time.time()
                    response_time = end_time - start_time
                    
                    print(f"\nğŸ“‹ æµ‹è¯•ç»“æœ:")
                    print(f"æ£€ç´¢æ–‡æ¡£æ•°: {len(relevant_docs)}")
                    print(f"å“åº”æ—¶é—´: {response_time:.2f}ç§’")
                    print(f"å›ç­”: {answer}")
                    
                    # æ˜¾ç¤ºæ£€ç´¢åˆ°çš„æ–‡æ¡£ç‰‡æ®µ
                    if relevant_docs:
                        print(f"\nğŸ” ç›¸å…³æ–‡æ¡£ç‰‡æ®µ:")
                        for i, doc in enumerate(relevant_docs[:2], 1):  # åªæ˜¾ç¤ºå‰2ä¸ª
                            print(f"[{i}] {doc.page_content[:100]}...")
                    
                except Exception as e:
                    print(f"âŒ å¤„ç†é—®é¢˜æ—¶å‡ºé”™: {e}")
        
        except Exception as e:
            print(f"âŒ åˆå§‹åŒ–æµ‹è¯•ç¯å¢ƒå¤±è´¥: {e}")


def main():
    """ä¸»å‡½æ•°"""
    if not os.getenv("OPENAI_API_KEY"):
        print("âŒ é”™è¯¯: è¯·è®¾ç½®OPENAI_API_KEYç¯å¢ƒå˜é‡")
        return
    
    evaluator = RAGEvaluator()
    
    while True:
        print("\nğŸš€ RAGç³»ç»Ÿè¯„ä¼°å·¥å…·")
        print("=" * 30)
        print("1. è¿è¡Œè‡ªåŠ¨è¯„ä¼°")
        print("2. äº¤äº’å¼æµ‹è¯•")
        print("3. é€€å‡º")
        
        choice = input("\nè¯·é€‰æ‹©æ“ä½œ (1-3): ").strip()
        
        if choice == "1":
            evaluator.run_evaluation()
        elif choice == "2":
            evaluator.run_interactive_test()
        elif choice == "3":
            print("å†è§ï¼")
            break
        else:
            print("æ— æ•ˆé€‰æ‹©ï¼Œè¯·é‡è¯•")


if __name__ == "__main__":
    main() 